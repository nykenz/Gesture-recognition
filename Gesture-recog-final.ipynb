{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition Case study\n",
    "#### By- Jeeva Thangaraj\n",
    "####        Nitesh Kumar Singh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "Imagine you are working as a data scientist at a home electronics company which manufactures state of the art smart televisions. You want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.\n",
    "\n",
    "The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
    " \n",
    "| Gesture | Corresponding Action |\n",
    "| --- | --- | \n",
    "| Thumbs Up | Increase the volume. |\n",
    "| Thumbs Down | Decrease the volume. |\n",
    "| Left Swipe | 'Jump' backwards 10 seconds. |\n",
    "| Right Swipe | 'Jump' forward 10 seconds. |\n",
    "| Stop | Pause the movie. |\n",
    "\n",
    "Each video is a sequence of 30 frames (or images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import cv2\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing some other libraries which will be needed for model building.\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization\n",
    "from keras.layers import Activation, Dropout, ZeroPadding3D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling3D, Conv3D, MaxPooling2D\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "rn.seed(30)\n",
    "tf.set_random_seed(30)\n",
    "train_path = './Project_data/train'\n",
    "val_path = './Project_data/val'\n",
    "train_doc = np.random.permutation(open('./Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('./Project_data/val.csv').readlines())\n",
    "batch_size = 10 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n",
    "batch_size = 10 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters initialization\n",
    "nb_rows = 120   # X dimension of the image\n",
    "nb_cols = 120   # Y dimesnion of the image\n",
    "total_frames = 30\n",
    "nb_frames = 30  # lenght of the video frames\n",
    "nb_channel = 3 # numbe rof channels in images 3 for color(RGB) and 1 for Gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator is broken down into modules. In the generator, we are going to preprocess the images as we have images of 2 different dimensions as well as create a batch of video frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_affine():\n",
    "    dx, dy = np.random.randint(-1.7, 1.8, 2)\n",
    "    M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_batch_data(batch_size):\n",
    "    batch_data = np.zeros((batch_size, nb_frames, nb_rows, nb_cols, nb_channel)) \n",
    "    batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "    return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_images(source_path, folder_list, batch_num, batch_size, t,validation):\n",
    "    \n",
    "    batch_data,batch_labels = init_batch_data(batch_size)\n",
    "    \n",
    "    # We will also build an augumented batch data with affine transformation\n",
    "    batch_data_aug,batch_labels_aug = init_batch_data(batch_size)\n",
    "    \n",
    "    # We will also build an augmented batch data with horizontal flip\n",
    "    batch_data_flip,batch_labels_flip = init_batch_data(batch_size)\n",
    "    \n",
    "    #create a list of image numbers you want to use for a particular video using full frames\n",
    "    img_idx = [x for x in range(0, nb_frames)] \n",
    "\n",
    "    for folder in range(batch_size): # iterate over the batch_size\n",
    "        # read all the images in the folder\n",
    "        imgs = sorted(os.listdir(source_path+'/'+ t[folder + (batch_num*batch_size)].split(';')[0])) \n",
    "        # Generate a random affine to be used in image transformation for buidling agumented data set\n",
    "        M = get_random_affine()\n",
    "        \n",
    "        #  Iterate over the frames/images of a folder to read them in\n",
    "        for idx, item in enumerate(img_idx): \n",
    "            ## image = imread(source_path+'/'+ t[folder + (batch_num*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "            image = cv2.imread(source_path+'/'+ t[folder + (batch_num*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Cropping non symmetric frames\n",
    "            if image.shape[0] != image.shape[1]:\n",
    "                image=image[0:120,20:140]\n",
    "            \n",
    "            #crop the images and resize them. Note that the images are of 2 different shape \n",
    "            #and the conv3D will throw error if the inputs in a batch have different shapes   \n",
    "            resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "            #Normal data\n",
    "            batch_data[folder,idx] = (resized)\n",
    "            \n",
    "            #Data with affine transformation\n",
    "            batch_data_aug[folder,idx] = (cv2.warpAffine(resized, M, (resized.shape[0], resized.shape[1])))\n",
    "            \n",
    "            # Data with horizontal flip\n",
    "            batch_data_flip[folder,idx]= np.flip(resized,1)\n",
    "\n",
    "        batch_labels[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "        batch_labels_aug[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "        \n",
    "        # Labeling data with horizobtal flip, right swipe becomes left swipe and viceversa\n",
    "        if int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==0:\n",
    "                    batch_labels_flip[folder, 1] = 1\n",
    "        elif int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==1:\n",
    "                    batch_labels_flip[folder, 0] = 1\n",
    "                    \n",
    "        else:\n",
    "                    batch_labels_flip[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "                  \n",
    "    \n",
    "    batch_data_final = np.append(batch_data, batch_data_aug, axis = 0)\n",
    "    batch_data_final = np.append(batch_data_final, batch_data_flip, axis = 0)\n",
    "\n",
    "    batch_labels_final = np.append(batch_labels, batch_labels_aug, axis = 0) \n",
    "    batch_labels_final = np.append(batch_labels_final, batch_labels_flip, axis = 0)\n",
    "    \n",
    "    if validation:\n",
    "        batch_data_final=batch_data\n",
    "        batch_labels_final= batch_labels\n",
    "        \n",
    "    return batch_data_final,batch_labels_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, validation=False):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            # you yield the batch_data and the batch_labels, remember what does yield do\n",
    "            yield load_batch_images(source_path, folder_list, batch, batch_size, t,validation)\n",
    "            \n",
    "\n",
    "        \n",
    "        # Code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            yield load_batch_images(source_path, folder_list, batch, batch_size, t,validation)\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to flip the image\n",
    "def __flip(self, image):\n",
    "        return np.flip(image,1)\n",
    "    \n",
    "# Helper function to normalise the data\n",
    "def __normalize(self, image):\n",
    "        return image/127.5-1\n",
    "    \n",
    "# Helper function to resize the image\n",
    "def __resize(self, image):\n",
    "        return cv2.resize(image, (self.width,self.height), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "# Helper function to crop the image\n",
    "def __crop(self, image):\n",
    "    if image.shape[0] != image.shape[1]:\n",
    "        return image[0:120, 20:140]\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filters = [8,16,32,64]\n",
    "nb_dense = [256, 128, 5]\n",
    "\n",
    "# Input\n",
    "input_shape=(nb_frames,nb_rows,nb_cols,nb_channel)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(nb_dense[0], activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(nb_dense[1], activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(nb_dense[2], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 864,101\n",
      "Trainable params: 863,989\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = Adam() #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc,batch_size=10)\n",
    "val_generator = generator(val_path, val_doc, batch_size=10,validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# write the Reducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_sequences//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_val_sequences/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/val ; batch size = 10\n",
      "Source path =  Epoch 1/20Project_data/train ; batch size = 10\n",
      "\n",
      "34/34 [==============================] - 96s 3s/step - loss: 1.8208 - categorical_accuracy: 0.2490 - val_loss: 1.6270 - val_categorical_accuracy: 0.1400\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-11-0911_47_58.096278/model-00001-1.82079-0.24902-1.62705-0.14000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 83s 2s/step - loss: 1.3002 - categorical_accuracy: 0.4649 - val_loss: 2.1534 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-11-0911_47_58.096278/model-00002-1.29705-0.46115-2.15343-0.46000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 14s 419ms/step - loss: 1.3766 - categorical_accuracy: 0.5163 - val_loss: 1.0882 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-11-0911_47_58.096278/model-00003-1.37665-0.51634-1.08821-0.58000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 23s 667ms/step - loss: 1.3990 - categorical_accuracy: 0.3889 - val_loss: 1.5246 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-11-0911_47_58.096278/model-00004-1.39897-0.38889-1.52463-0.22000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 20s 590ms/step - loss: 1.4559 - categorical_accuracy: 0.4085 - val_loss: 2.0520 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-11-0911_47_58.096278/model-00005-1.45590-0.40850-2.05202-0.26000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 19s 548ms/step - loss: 1.1467 - categorical_accuracy: 0.5261 - val_loss: 2.4692 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-11-0911_47_58.096278/model-00006-1.14665-0.52614-2.46915-0.22000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 16s 459ms/step - loss: 1.1774 - categorical_accuracy: 0.5229 - val_loss: 1.0380 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-11-0911_47_58.096278/model-00007-1.17736-0.52288-1.03800-0.60000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 14s 415ms/step - loss: 1.0457 - categorical_accuracy: 0.5294 - val_loss: 1.2047 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-11-0911_47_58.096278/model-00008-1.04569-0.52941-1.20468-0.50000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 15s 449ms/step - loss: 1.0120 - categorical_accuracy: 0.6176 - val_loss: 1.1109 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-11-0911_47_58.096278/model-00009-1.01201-0.61765-1.11086-0.56000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 14s 406ms/step - loss: 0.9671 - categorical_accuracy: 0.5654 - val_loss: 0.8823 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-11-0911_47_58.096278/model-00010-0.96708-0.56536-0.88228-0.70000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 14s 409ms/step - loss: 0.9057 - categorical_accuracy: 0.6634 - val_loss: 0.8413 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-11-0911_47_58.096278/model-00011-0.90569-0.66340-0.84127-0.68000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 16s 456ms/step - loss: 0.8578 - categorical_accuracy: 0.6569 - val_loss: 0.7367 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-11-0911_47_58.096278/model-00012-0.85783-0.65686-0.73667-0.72000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 15s 439ms/step - loss: 0.8152 - categorical_accuracy: 0.6765 - val_loss: 0.8919 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-11-0911_47_58.096278/model-00013-0.81523-0.67647-0.89193-0.70000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 15s 442ms/step - loss: 0.8913 - categorical_accuracy: 0.6503 - val_loss: 0.8181 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-11-0911_47_58.096278/model-00014-0.89132-0.65033-0.81810-0.64000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 14s 407ms/step - loss: 0.8003 - categorical_accuracy: 0.6732 - val_loss: 0.6960 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-11-0911_47_58.096278/model-00015-0.80034-0.67320-0.69600-0.70000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 15s 429ms/step - loss: 0.7965 - categorical_accuracy: 0.6569 - val_loss: 0.8813 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-11-0911_47_58.096278/model-00016-0.79649-0.65686-0.88126-0.68000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 15s 437ms/step - loss: 0.5951 - categorical_accuracy: 0.7582 - val_loss: 0.7474 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-11-0911_47_58.096278/model-00017-0.59514-0.75817-0.74736-0.72000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 14s 419ms/step - loss: 0.8094 - categorical_accuracy: 0.7026 - val_loss: 0.7747 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-11-0911_47_58.096278/model-00018-0.80944-0.70261-0.77467-0.68000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 15s 441ms/step - loss: 0.6710 - categorical_accuracy: 0.7549 - val_loss: 0.6746 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-11-0911_47_58.096278/model-00019-0.67103-0.75490-0.67461-0.76000.h5\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 14s 420ms/step - loss: 0.6791 - categorical_accuracy: 0.7418 - val_loss: 0.8269 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-11-0911_47_58.096278/model-00020-0.67910-0.74183-0.82688-0.70000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe88100d4a8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "    axes[0].plot(history.history['loss'])   \n",
    "    axes[0].plot(history.history['val_loss'])\n",
    "    axes[0].legend(['loss','val_loss'])\n",
    "\n",
    "    axes[1].plot(history.history['categorical_accuracy'])   \n",
    "    axes[1].plot(history.history['val_categorical_accuracy'])\n",
    "    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-939dfb022365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "plot(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from keras.models import load_model\n",
    "model = load_model('model_init_2020-11-0911_47_58.096278/model-00020-0.67910-0.74183-0.82688-0.70000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator=model()\n",
    "test_generator.initialize_path(project_folder)\n",
    "test_generator.initialize_image_properties(image_height=120,image_width=120)\n",
    "test_generator.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n",
    "\n",
    "g=test_generator.generator(test_generator.val_path,test_generator.val_doc,augment=False)\n",
    "batch_data, batch_labels=next(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
